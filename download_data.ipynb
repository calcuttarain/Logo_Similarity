{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c064fa",
   "metadata": {},
   "source": [
    "# Download Logos\n",
    "\n",
    "Acest notebook downloadeaza logo-urile in folderul 'dataset/logos_raw/', fara domeniile duplicate din fisier. Logo-urile acceptate sunt cele care contin in denumire cuvantul logo si au ca extensie .png, .jpg, .jpeg, .webp sau .svg. Am incercat sa paralelizez procesul trimitand pachetele de cereri prin mai multe fire de executie (120) simultan. Pentru fiecare domeniu, pipeline-ul a fost urmatorul:\n",
    "1. sa extrag html-ul site-ului\n",
    "2. sa gasesc calea fisierului de logo\n",
    "3. sa downloadez fisierul de logo\n",
    "\n",
    "In principiu, au fost 3 probleme cu care m-am confruntat:\n",
    "- era acceptata la putine site-uri orice fel de cerere. Am rezolvat-o cat am putut de mult uitandu-ma pe erorile pe care le primeam de la acele site-uri si imbunatatind headerul pachetului si ritmul cererilor. Imbunatiri notabile au fost: randomizarea User_Agent si Accept-Language, retry cu backoff, reincercarea link-urilor la anumite coduri de eroare http, fallback la cloudscraper daca nu functiona metoda din libraria request, pastrarea cookie-urilor si sesiunilor prin session = requests.Session() si permiterea redirectionarii la alte pagini session.get(url, timeout=10, allow_redirects=True)\n",
    "- a fost greu de gasit link-ul fisierului de logo. Am folosit BeautifulSoup pentru parsarea html-ului, am incercat metode euristice.\n",
    "- era acceptata de putine site-uri functia de download, dar am abordat problema in aceeasi maniera in care am abordat-o pe prima\n",
    "\n",
    "O imbunatatire buna ar fi implementarea unei logici de fallback la un browser headless, cum ar fi selenium pentru site-urile la care e necesara executie javascript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c865dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openpyxl\n",
    "import random\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from fake_useragent import UserAgent\n",
    "import cloudscraper\n",
    "import os\n",
    "from enum import Enum, auto\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5391ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "PROTOCOLS = ['https://', \n",
    "             'http://',\n",
    "]\n",
    "\n",
    "HEADERS = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,\"\n",
    "                  \"image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Sec-CH-UA\": '\"Chromium\";v=\"124\", \"Google Chrome\";v=\"124\", \"Not-A.Brand\";v=\"99\"',\n",
    "}\n",
    "\n",
    "ACCEPT_LANGS = [\n",
    "    \"en-US,en;q=0.9\", \n",
    "    \"ro-RO,ro;q=0.9,en-US;q=0.8\", \n",
    "    \"fr-FR,fr;q=0.9,en;q=0.8\",\n",
    "    \"de-DE,de;q=0.9,en;q=0.8\", \n",
    "    \"es-ES,es;q=0.9,en;q=0.7\", \n",
    "]\n",
    "\n",
    "LOGO_FOLDER = 'dataset/logos_raw/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5dfa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogoError(Enum):\n",
    "    OK = auto()\n",
    "    PAGE_LOAD_FAIL = auto()\n",
    "    NO_LOGO_FOUND = auto()\n",
    "    DOWNLOAD_LOGO_FAIL = auto()\n",
    "    SAVE_FAIL = auto()\n",
    "    UNKNOWN = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67eb4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logo_url(html, base):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        extensions = ('.png', '.jpg', '.jpeg', '.svg', '.webp')\n",
    "\n",
    "        for img in soup.find_all('img'):\n",
    "            for attr in ['src', 'data-src', 'data-lazy-src', 'srcset']:\n",
    "                src_raw = img.get(attr)\n",
    "                if not src_raw:\n",
    "                    continue\n",
    "\n",
    "                src_raw = src_raw.strip()\n",
    "                if not src_raw or src_raw.startswith(\"data:\"):\n",
    "                    continue\n",
    "\n",
    "                if \" \" in src_raw:\n",
    "                    src_raw = src_raw.split()[0]\n",
    "\n",
    "                src_lower = src_raw.lower()\n",
    "\n",
    "                if 'logo' in src_lower and any(src_lower.endswith(ext) for ext in extensions):\n",
    "                    return urljoin(base, src_raw)\n",
    "\n",
    "        for source in soup.find_all('source'):\n",
    "            for attr in ['srcset', 'data-srcset']:\n",
    "                src_raw = source.get(attr)\n",
    "                if not src_raw:\n",
    "                    continue\n",
    "\n",
    "                src_raw = src_raw.strip().split()[0]\n",
    "                src_lower = src_raw.lower()\n",
    "\n",
    "                if 'logo' in src_lower and any(src_lower.endswith(ext) for ext in extensions):\n",
    "                    return urljoin(base, src_raw)\n",
    "\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[find_logo_url error] {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d682632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_logo(domain, url, session=None, max_retries=3, base_delay=1.5):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    if not session:\n",
    "        session = requests.Session()\n",
    "        session.headers.update(HEADERS)\n",
    "\n",
    "    session.headers[\"User-Agent\"] = ua.random\n",
    "    session.headers[\"Accept-Language\"] = random.choice(ACCEPT_LANGS)\n",
    "\n",
    "    last_err = None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = session.get(url, stream=True, timeout=10, allow_redirects=True)\n",
    "\n",
    "            if resp.status_code in (429, 502, 503, 504):\n",
    "                last_err = f\"HTTP {resp.status_code}\"\n",
    "                delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0.5, 1.5)\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "\n",
    "            if resp.status_code != 200:\n",
    "                # eroare non-retryable \n",
    "                return False, f\"HTTP {resp.status_code} - {resp.reason}\"\n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "            basename = os.path.basename(parsed_url.path)\n",
    "\n",
    "            safe_domain = domain.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "            filename = f\"{safe_domain}_{basename}\"\n",
    "\n",
    "            filepath = os.path.join(LOGO_FOLDER, filename)\n",
    "            os.makedirs(LOGO_FOLDER, exist_ok=True)\n",
    "\n",
    "            response = session.get(url, stream=True, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            return True, ' '\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            last_err = str(e)\n",
    "            # backoff exponential + jitter\n",
    "            delay = base_delay * (2 ** (attempt - 1)) + random.uniform(0.5, 1.5)\n",
    "            time.sleep(delay)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # alte erori (IO etc.)\n",
    "            return False, str(e)\n",
    "\n",
    "    # dacă am epuizat retry-urile\n",
    "    return False, (last_err or \"Unknown error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131145a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(domain, session = None):\n",
    "\n",
    "    result = {\n",
    "        'domain': domain,\n",
    "        'url': None,\n",
    "        'success': False,\n",
    "        'error_type': None,\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    error = None\n",
    "\n",
    "    ua = UserAgent() \n",
    "\n",
    "    if not session:\n",
    "        session = requests.Session()\n",
    "        session.headers.update(HEADERS)\n",
    "\n",
    "    session.headers[\"User-Agent\"] = ua.random\n",
    "    session.headers[\"Accept-Language\"] = random.choice(ACCEPT_LANGS)\n",
    "\n",
    "    max_retries = 3\n",
    "    base_delay = 1.5 \n",
    "\n",
    "    for protocol in PROTOCOLS:\n",
    "        url = protocol + domain\n",
    "        result['url'] = url\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                response = session.get(url, timeout=10, allow_redirects=True)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "\n",
    "                    logo_url = find_logo_url(response.text, url)\n",
    "                    if not logo_url:\n",
    "                        result['error_type'] = LogoError.NO_LOGO_FOUND\n",
    "                        return result\n",
    "\n",
    "                    ok_download_logo, err = download_logo(domain, logo_url, session)\n",
    "                    if not ok_download_logo:\n",
    "                        result['error_type'], result['error'] = LogoError.DOWNLOAD_LOGO_FAIL, err\n",
    "                        return result\n",
    "\n",
    "                    result['success'], result['error_type'] = True, LogoError.OK\n",
    "                    return result\n",
    "\n",
    "                if response.status_code in (429, 503, 502, 504):\n",
    "                    delay = base_delay * attempt + random.uniform(0.5, 1.5)\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "\n",
    "                error = f\"HTTP {response.status_code}\"\n",
    "                break\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                error = str(e)\n",
    "                delay = base_delay * attempt + random.uniform(0.5, 1.5)\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "\n",
    "    # cloudscraper extinde session\n",
    "    if not isinstance(session, cloudscraper.CloudScraper):\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        scraper.headers.update(HEADERS)\n",
    "        return request_html(domain, scraper)\n",
    "\n",
    "    result['error_type'], result['error'] = LogoError.PAGE_LOAD_FAIL, error\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesare site-uri: 100%|██████████| 3416/3416 [09:20<00:00,  6.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Site-uri succes: 2356\n",
      "Site-uri la care nu a fost succes: 1060\n",
      "Procentul de site-uri succes: 68.97%\n",
      "Timp total: 580.27 secunde\n",
      "Erorile au fost salvate în 'errors.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# extract html\n",
    "df = pd.read_parquet(\"./logos.snappy.parquet\", engine='fastparquet')\n",
    "df = df.drop_duplicates(subset='domain', keep='first')\n",
    "\n",
    "domains = df['domain'].tolist()\n",
    "reached = 0\n",
    "not_reached = 0\n",
    "errors = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=120) as executor:\n",
    "    futures = [executor.submit(request_html, d) for d in domains]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Procesare site-uri\"):\n",
    "        result = future.result()\n",
    "        if result['success']:\n",
    "            reached += 1\n",
    "        else:\n",
    "            errors.append(result)\n",
    "            not_reached += 1\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "percent_reached = (reached / len(domains)) * 100\n",
    "\n",
    "print(f'\\nSite-uri succes: {reached}')\n",
    "print(f'Site-uri la care nu a fost succes: {not_reached}')\n",
    "print(f'Procentul de site-uri succes: {percent_reached:.2f}%')\n",
    "print(f'Timp total: {elapsed:.2f} secunde')\n",
    "\n",
    "\n",
    "if errors:\n",
    "    errors_df = pd.DataFrame(errors)\n",
    "    errors_df.to_excel('dataset/errors.xlsx', index=False)\n",
    "    print(\"Erorile au fost salvate în 'errors.xlsx'\")\n",
    "else:\n",
    "    print(\"Nu au fost erori de salvat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826e0d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ ERROR SUMMARY ================\n",
      "PAGE_LOAD_FAIL       : 311\n",
      "NO_LOGO_FOUND        : 723\n",
      "DOWNLOAD_LOGO_FAIL   : 26\n",
      "Fail              : 1060\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_counts = Counter(e['error_type'] for e in errors if e['error_type'])\n",
    "\n",
    "print(\"\\n================ ERROR SUMMARY ================\")\n",
    "for err_type, count in error_counts.items():\n",
    "    print(f\"{err_type.name:<20} : {count}\")\n",
    "\n",
    "total = len(errors)\n",
    "print(f\"Fail              : {total}\")\n",
    "print(\"==============================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".logo_similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
